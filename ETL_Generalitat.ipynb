{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las Cortes\n",
    "\n",
    "Scraping: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPEA Y GUARDA EL HTML DE RESULTADOS DE CADA ELECCION CON SOLO INFO DE LAS CORTES\n",
    "# ASSET PARA DESPUES SACAR LA INFO EN JSON\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import os\n",
    "\n",
    "# üìå Configurar Selenium en modo headless (sin interfaz gr√°fica)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# üìå Iniciar WebDriver autom√°ticamente con WebDriver Manager\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# üìå Diccionario con los a√±os y URLs de las elecciones a las Cortes Valencianas en Wikipedia\n",
    "election_urls = {\n",
    "    \"1983\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_1983\",\n",
    "    \"1987\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_1987\",\n",
    "    \"1991\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_1991\",\n",
    "    \"1995\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_1995\",\n",
    "    \"1999\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_1999\",\n",
    "    \"2003\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_2003\",\n",
    "    \"2007\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_2007\",\n",
    "    \"2011\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_2011\",\n",
    "    \"2015\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_2015\",\n",
    "    \"2019\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_2019\",\n",
    "    \"2023\": \"https://es.wikipedia.org/wiki/Elecciones_a_las_Cortes_Valencianas_de_2023\"\n",
    "}\n",
    "\n",
    "# üìå Carpeta donde se guardar√°n los archivos HTML\n",
    "output_folder = \"elecciones_html/\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# üìå Recorrer todas las URLs y guardar el HTML completo\n",
    "for year, url in election_urls.items():\n",
    "    print(f\"üîç Guardando HTML de {year}...\")\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(4)  # Esperar para que cargue la p√°gina\n",
    "\n",
    "    # Obtener el HTML completo de la p√°gina\n",
    "    page_html = driver.page_source\n",
    "\n",
    "    # Guardar el HTML en un archivo\n",
    "    html_path = os.path.join(output_folder, f\"elecciones_{year}.html\")\n",
    "    with open(html_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(page_html)\n",
    "\n",
    "    print(f\"‚úÖ Guardado: {html_path}\")\n",
    "\n",
    "# üìå Cerrar Selenium\n",
    "driver.quit()\n",
    "print(\"\\nüöÄ Descarga completada. Archivos guardados en la carpeta 'elecciones_html/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEE LOS ARCHIVOS HTML LIMPIOS CON SOLO LA TABLA Y \n",
    "# GENERA UN JSON PARA LOS DIPUTADOS DE CADA LEGISLATURA\n",
    "# EN UN NUEVO FOLDER ./resultados_json\n",
    "\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Directorio donde est√°n los archivos HTML\n",
    "folder_path = \"./\"\n",
    "output_folder = os.path.join(folder_path, \"resultados_json\")\n",
    "\n",
    "# Crear el folder de salida si no existe\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Obtener la lista de archivos HTML en la carpeta\n",
    "html_files = [f for f in os.listdir(folder_path) if f.endswith(\".html\")]\n",
    "\n",
    "# Iterar sobre cada archivo HTML\n",
    "for html_file in html_files:\n",
    "    file_path = os.path.join(folder_path, html_file)\n",
    "\n",
    "    # Leer el archivo HTML\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    # Encontrar todas las filas de la tabla\n",
    "    tbody = soup.find(\"tbody\")\n",
    "    if not tbody:\n",
    "        print(f\"No se encontr√≥ tbody en {html_file}. Saltando...\")\n",
    "        continue\n",
    "    \n",
    "    rows = tbody.find_all(\"tr\")\n",
    "\n",
    "    diputados = []\n",
    "    current_party = None\n",
    "\n",
    "    # Iterar sobre las filas para extraer los datos\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        \n",
    "        if len(cols) >= 2:\n",
    "            if cols[1].b:  # Identificar el partido\n",
    "                current_party = cols[1].get_text(strip=True).split(\"\\n\")[0]\n",
    "                num_diputados = int(cols[1].get_text(strip=True).split(\"(\")[-1].split()[0])\n",
    "            else:\n",
    "                num_diputados = None\n",
    "            \n",
    "            # Obtener nombres de diputados por circunscripci√≥n\n",
    "            circunscripciones = [\"Alicante\", \"Castell√≥n\", \"Valencia\"]\n",
    "            diputados_por_circunscripcion = {}\n",
    "\n",
    "            for i, circ in enumerate(circunscripciones):\n",
    "                if len(cols) > i + 2:\n",
    "                    diputados_lista = [li.get_text(strip=True) for li in cols[i + 2].find_all(\"li\")]\n",
    "                    diputados_por_circunscripcion[circ] = diputados_lista\n",
    "\n",
    "            # Guardar los datos\n",
    "            diputados.append({\n",
    "                \"partido\": current_party,\n",
    "                \"total_diputados\": num_diputados,\n",
    "                \"diputados_por_circunscripcion\": diputados_por_circunscripcion\n",
    "            })\n",
    "\n",
    "    # Definir el nombre del archivo JSON dentro de la carpeta de resultados\n",
    "    json_file_name = html_file.replace(\".html\", \".json\")\n",
    "    json_file_path = os.path.join(output_folder, json_file_name)\n",
    "\n",
    "    # Guardar en JSON dentro del folder\n",
    "    with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(diputados, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Archivo JSON generado con √©xito: {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEE LOS JSON Y UNIFICA EN UN SOLO CSV\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Diccionario con las legislaturas y sus rangos de a√±os\n",
    "legislaturas = {\n",
    "    \"I\": (1983, 1987),\n",
    "    \"II\": (1987, 1991),\n",
    "    \"III\": (1991, 1995),\n",
    "    \"IV\": (1995, 1999),\n",
    "    \"V\": (1999, 2003),\n",
    "    \"VI\": (2003, 2007),\n",
    "    \"VII\": (2007, 2011),\n",
    "    \"VIII\": (2011, 2015),\n",
    "    \"IX\": (2015, 2019),\n",
    "    \"X\": (2019, 2023),\n",
    "    \"XI\": (2023, 2025)  # 2025 como \"actualidad\" temporal\n",
    "}\n",
    "\n",
    "# Directorio donde est√°n los archivos JSON\n",
    "folder_path = \"./resultados_json\"\n",
    "output_file = \"diputados_consolidado.csv\"\n",
    "\n",
    "# Lista para almacenar los datos\n",
    "datos = []\n",
    "\n",
    "# Obtener la lista de archivos JSON en la carpeta\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "\n",
    "# Iterar sobre cada archivo JSON\n",
    "for json_file in json_files:\n",
    "    # Extraer el a√±o desde el nombre del archivo (asumiendo formato 'diputados_AAAA.json')\n",
    "    year = int(json_file.split(\"_\")[1].split(\".\")[0])\n",
    "\n",
    "    # Determinar la legislatura correspondiente\n",
    "    legislatura_asignada = None\n",
    "    for legislatura, (inicio, fin) in legislaturas.items():\n",
    "        if inicio <= year < fin:\n",
    "            legislatura_asignada = legislatura\n",
    "            break\n",
    "\n",
    "    # Leer el archivo JSON\n",
    "    with open(os.path.join(folder_path, json_file), \"r\", encoding=\"utf-8\") as file:\n",
    "        diputados_data = json.load(file)\n",
    "\n",
    "    # Extraer datos y agregarlos a la lista\n",
    "    for partido in diputados_data:\n",
    "        for provincia, diputados in partido[\"diputados_por_circunscripcion\"].items():\n",
    "            for diputado in diputados:\n",
    "                datos.append([diputado, partido[\"partido\"], provincia, legislatura_asignada])\n",
    "\n",
    "# Guardar los datos en un CSV\n",
    "with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=\";\")\n",
    "    writer.writerow([\"diputado\", \"partido\", \"provincia\", \"n_legislatura\"])  # Cabecera\n",
    "    writer.writerows(datos)\n",
    "\n",
    "print(f\"Archivo CSV generado con √©xito: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consellers (Wayback Machine)\n",
    "\n",
    "Scraping: Wayback Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve un JSON con la info de consellers hasta la Legislatura IX\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import time\n",
    "\n",
    "# URL de la versi√≥n archivada\n",
    "URL = \"https://web.archive.org/web/20150923221316/http://www.argos.gva.es/va/institucional/consell/\"\n",
    "\n",
    "# Configurar Selenium con ChromeDriver Manager\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Ejecutar sin interfaz gr√°fica\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "driver.get(URL)\n",
    "\n",
    "# Esperar que la p√°gina cargue completamente\n",
    "time.sleep(5)\n",
    "\n",
    "# Intentar encontrar los botones de legislaturas\n",
    "try:\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    legislaturas = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.tab-menu li a\")))\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for legislatura in legislaturas:\n",
    "        nombre_legislatura = legislatura.text.strip()\n",
    "        print(f\"Extrayendo datos de {nombre_legislatura}...\")\n",
    "\n",
    "        # Usar JavaScript para hacer clic en la legislatura\n",
    "        driver.execute_script(\"arguments[0].click();\", legislatura)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Buscar las secciones de cada equipo de gobierno\n",
    "        equipos_gobierno = driver.find_elements(By.CSS_SELECTOR, \"table.tabla_demoscopia\")\n",
    "        data[nombre_legislatura] = []\n",
    "\n",
    "        for equipo in equipos_gobierno:\n",
    "            try:\n",
    "                titulo_equipo = equipo.find_element(By.XPATH, \"./tbody/tr[1]/td\").text.strip()\n",
    "                consejeros = equipo.find_elements(By.CSS_SELECTOR, \"td ul li\")\n",
    "                lista_consejeros = [c.text.strip() for c in consejeros if c.text.strip()]\n",
    "                \n",
    "                if titulo_equipo and lista_consejeros:\n",
    "                    data[nombre_legislatura].append({\n",
    "                        \"equipo\": titulo_equipo,\n",
    "                        \"consejeros\": lista_consejeros\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extrayendo equipo de gobierno en {nombre_legislatura}: {e}\")\n",
    "\n",
    "    # Eliminar legislaturas vac√≠as\n",
    "    data = {k: v for k, v in data.items() if v}\n",
    "\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    with open(\"consejeros_valencia.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"Extracci√≥n completada. Datos guardados en 'consejeros_valencia.json'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"No se encontraron los elementos esperados. Error:\", e)\n",
    "    print(\"HTML de la p√°gina para depuraci√≥n:\\n\", driver.page_source)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consellers (pag. Oficial Govern)\n",
    "\n",
    "Scraping: Govern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POR FIN ITERA√áAO BUENA - RECOGE DATOS DE LOS CONSEJEROS DE LA PAG DEL GOVERN\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_gobierno_info(url, nombre_legislatura):\n",
    "    \"\"\"\n",
    "    Extrae la informaci√≥n de los gobiernos de una legislatura espec√≠fica\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL de la p√°gina de la legislatura\n",
    "        nombre_legislatura (str): Nombre o identificador de la legislatura\n",
    "    \n",
    "    Returns:\n",
    "        dict: Informaci√≥n de la legislatura y sus gobiernos\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Realizar la solicitud HTTP\n",
    "    print(f\"Obteniendo informaci√≥n de: {nombre_legislatura} - {url}\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Verificar si la solicitud fue exitosa\n",
    "    if response.status_code != 200:\n",
    "        return {\"error\": f\"Error al acceder a la p√°gina {nombre_legislatura}: {response.status_code}\"}\n",
    "    \n",
    "    # Parsear el contenido HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Inicializar la estructura para almacenar la informaci√≥n\n",
    "    legislatura = {\n",
    "        \"nombre\": nombre_legislatura,\n",
    "        \"url\": url,\n",
    "        \"gobiernos\": []\n",
    "    }\n",
    "    \n",
    "    # Buscar todos los gobiernos\n",
    "    gobiernos_divs = soup.find_all('div', class_='governs')\n",
    "    \n",
    "    if not gobiernos_divs:\n",
    "        return {\"nombre\": nombre_legislatura, \"url\": url, \"error\": \"No se encontraron gobiernos en la p√°gina\"}\n",
    "    \n",
    "    # Iterar sobre cada gobierno\n",
    "    for gobierno_div in gobiernos_divs:\n",
    "        gobierno_info = {}\n",
    "        \n",
    "        # Extraer el t√≠tulo/per√≠odo del gobierno\n",
    "        titulo_gobierno = gobierno_div.find('h2', class_='asset-title')\n",
    "        if titulo_gobierno and titulo_gobierno.a:\n",
    "            # Eliminar el onClick y extraer solo el texto\n",
    "            gobierno_info[\"titulo\"] = titulo_gobierno.a.text.strip()\n",
    "            \n",
    "            # Intentar extraer fechas del t√≠tulo usando regex\n",
    "            fecha_match = re.search(r'(\\d{2}\\.\\d{2}\\.\\d{4})-(\\d{2}\\.\\d{2}\\.\\d{4})', gobierno_info[\"titulo\"])\n",
    "            if fecha_match:\n",
    "                gobierno_info[\"fecha_inicio\"] = fecha_match.group(1)\n",
    "                gobierno_info[\"fecha_fin\"] = fecha_match.group(2)\n",
    "        \n",
    "        # Encontrar el contenedor de informaci√≥n del gobierno\n",
    "        gobierno_container = gobierno_div.find('div', class_='governsGva')\n",
    "        if not gobierno_container:\n",
    "            continue\n",
    "        \n",
    "        # Extraer informaci√≥n del presidente\n",
    "        presidente_div = gobierno_container.find('div', class_='presidente')\n",
    "        if presidente_div:\n",
    "            presidente_info = {}\n",
    "            \n",
    "            # Extraer nombre del presidente\n",
    "            nombre_span = presidente_div.find('span', class_='nombre')\n",
    "            if nombre_span:\n",
    "                presidente_info[\"nombre\"] = nombre_span.text.strip()\n",
    "            \n",
    "            # Extraer cargo del presidente\n",
    "            cargo_p = presidente_div.find('p', class_='cargo')\n",
    "            if cargo_p:\n",
    "                presidente_info[\"cargo\"] = cargo_p.text.strip()\n",
    "            \n",
    "            gobierno_info[\"presidente\"] = presidente_info\n",
    "        \n",
    "        # Extraer informaci√≥n de los consejeros\n",
    "        consejeros = []\n",
    "        consellers_divs = gobierno_container.find_all('div', class_='conseller')\n",
    "        \n",
    "        for conseller_div in consellers_divs:\n",
    "            consejero_info = {}\n",
    "            \n",
    "            # Extraer nombre del consejero\n",
    "            nombre_span = conseller_div.find('span', class_='nombre')\n",
    "            if nombre_span:\n",
    "                consejero_info[\"nombre\"] = nombre_span.text.strip()\n",
    "            \n",
    "            # Extraer cargo del consejero\n",
    "            cargo_p = conseller_div.find('p', class_='cargo')\n",
    "            if cargo_p:\n",
    "                # El cargo puede contener un enlace o ser texto plano\n",
    "                cargo_link = cargo_p.find('a')\n",
    "                if cargo_link:\n",
    "                    consejero_info[\"cargo\"] = cargo_link.text.strip()\n",
    "                    consejero_info[\"enlace_nombramiento\"] = cargo_link['href']\n",
    "                else:\n",
    "                    consejero_info[\"cargo\"] = cargo_p.text.strip()\n",
    "            \n",
    "            # A√±adir el consejero a la lista\n",
    "            if consejero_info:\n",
    "                consejeros.append(consejero_info)\n",
    "        \n",
    "        gobierno_info[\"consejeros\"] = consejeros\n",
    "        \n",
    "        # A√±adir el gobierno a la lista de gobiernos\n",
    "        legislatura[\"gobiernos\"].append(gobierno_info)\n",
    "    \n",
    "    return legislatura\n",
    "\n",
    "def main():\n",
    "    # Lista de URLs de las legislaturas\n",
    "    legislaturas = [\n",
    "        {\"url\": \"https://presidencia.gva.es/es/govern-preautonomic\", \"nombre\": \"Gobierno Preauton√≥mico\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/govern-de-transicio\", \"nombre\": \"Gobierno de Transici√≥n\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/i-legislatura\", \"nombre\": \"I Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/ii-legislatura\", \"nombre\": \"II Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/iii-legislatura\", \"nombre\": \"III Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/iv-legislatura\", \"nombre\": \"IV Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/v-legislatura\", \"nombre\": \"V Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/vi-legislatura\", \"nombre\": \"VI Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/vii-legislatura\", \"nombre\": \"VII Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/viii-legislatura\", \"nombre\": \"VIII Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/ix-legislatura\", \"nombre\": \"IX Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/x-legislatura\", \"nombre\": \"X Legislatura\"},\n",
    "        {\"url\": \"https://presidencia.gva.es/es/xi-legislatura\", \"nombre\": \"XI Legislatura\"}\n",
    "    ]\n",
    "    \n",
    "    # Crear un directorio con timestamp para almacenar los resultados\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"gobiernos_gva_{timestamp}\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Creado directorio para resultados: {output_dir}\")\n",
    "    \n",
    "    # Inicializar la estructura para almacenar toda la informaci√≥n\n",
    "    all_legislaturas = []\n",
    "    \n",
    "    # Iterar sobre cada legislatura\n",
    "    for legislatura in legislaturas:\n",
    "        # Obtener la informaci√≥n de la legislatura\n",
    "        info = get_gobierno_info(legislatura[\"url\"], legislatura[\"nombre\"])\n",
    "        \n",
    "        # Guardar la informaci√≥n en un archivo JSON individual\n",
    "        filename = f\"{output_dir}/{legislatura['nombre'].replace(' ', '_').lower()}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(info, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Informaci√≥n guardada en: {filename}\")\n",
    "        \n",
    "        # A√±adir la informaci√≥n a la lista completa\n",
    "        all_legislaturas.append(info)\n",
    "        \n",
    "        # Esperar un poco entre solicitudes para no sobrecargar el servidor\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Guardar toda la informaci√≥n en un √∫nico archivo JSON\n",
    "    with open(f\"{output_dir}/todos_los_gobiernos_gva.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_legislaturas, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Guardar un resumen estad√≠stico\n",
    "    stats = {\n",
    "        \"fecha_ejecucion\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"total_legislaturas\": len(legislaturas),\n",
    "        \"legislaturas_procesadas\": len(all_legislaturas),\n",
    "        \"total_gobiernos\": sum(len(leg.get(\"gobiernos\", [])) for leg in all_legislaturas),\n",
    "        \"errores\": [leg[\"nombre\"] for leg in all_legislaturas if \"error\" in leg]\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_dir}/resumen.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(\"\\nProceso completado.\")\n",
    "    print(f\"Toda la informaci√≥n ha sido guardada en el directorio: {output_dir}\")\n",
    "    print(f\"Archivos generados:\")\n",
    "    print(f\"- {len(legislaturas)} archivos individuales por legislatura\")\n",
    "    print(f\"- Un archivo completo con todas las legislaturas: todos_los_gobiernos_gva.json\")\n",
    "    print(f\"- Un archivo de resumen: resumen.json\")\n",
    "    \n",
    "    # Mostrar las estad√≠sticas\n",
    "    print(\"\\nResumen:\")\n",
    "    print(f\"- Total de legislaturas procesadas: {stats['legislaturas_procesadas']}\")\n",
    "    print(f\"- Total de gobiernos extra√≠dos: {stats['total_gobiernos']}\")\n",
    "    if stats['errores']:\n",
    "        print(f\"- Legislaturas con errores: {', '.join(stats['errores'])}\")\n",
    "    else:\n",
    "        print(\"- No se encontraron errores durante el proceso\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESA JSON DE ENTRADA (CONSELL) Y TRANSFORMA A CSV DE SALIDA\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Nombre de los archivos\n",
    "json_file = \"todos_los_gobiernos_gva.json\"\n",
    "csv_file = \"consells_consolidado.csv\"\n",
    "\n",
    "# Cargar los datos del JSON\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Lista para almacenar los datos extra√≠dos\n",
    "rows = []\n",
    "\n",
    "# Iterar sobre cada legislatura\n",
    "for legislatura in data:\n",
    "    n_legislatura = legislatura[\"nombre\"]  # Nombre de la legislatura\n",
    "\n",
    "    # Iterar sobre cada equipo de gobierno dentro de la legislatura\n",
    "    for gov in legislatura[\"gobiernos\"]:\n",
    "        n_govern = gov[\"titulo\"]  # T√≠tulo del gobierno\n",
    "\n",
    "        # Incluir al presidente en la lista\n",
    "        presidente = gov[\"presidente\"]\n",
    "        rows.append([n_legislatura, n_govern, presidente[\"nombre\"], presidente[\"cargo\"]])\n",
    "\n",
    "        # Iterar sobre cada consejero\n",
    "        for consejero in gov[\"consejeros\"]:\n",
    "            rows.append([n_legislatura, n_govern, consejero[\"nombre\"], consejero[\"cargo\"]])\n",
    "\n",
    "# Guardar en CSV\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file, delimiter=\";\")\n",
    "    writer.writerow([\"n_legislatura\", \"n_govern\", \"persona\", \"cargo\"])  # Encabezados\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"‚úÖ Archivo CSV generado: {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
